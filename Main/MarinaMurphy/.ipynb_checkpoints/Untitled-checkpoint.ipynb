{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import time\n",
    "import pyblp\n",
    "import auxiliary as aux\n",
    "import sqldf\n",
    "import pysqldf as ps\n",
    "from pandasql import sqldf\n",
    "import pandasql\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from clean_data import clean_data\n",
    "\n",
    "def int_to_month(value):\n",
    "        year = np.floor((value - 1) / 12)\n",
    "        month = value - 12 * year\n",
    "        return year, month\n",
    "\n",
    "def parse_info(code):\n",
    "    file = open('../../../../All/m_' + code + '/info.txt', mode = 'r')\n",
    "    info_file = file.read()\n",
    "    file.close()\n",
    "\n",
    "    all_info_elements = re.finditer('\\[(.*?):(.*?)\\]', info_file, re.DOTALL)\n",
    "    info_dict = {}\n",
    "    for info in all_info_elements:\n",
    "        info_name = info.group(1).strip()\n",
    "        info_content = info.group(2).strip()\n",
    "        info_dict[info_name] = info_content\n",
    "    return info_dict\n",
    "\n",
    "def get_date_range(initial_year_string, final_year_string, pre_months = 24, post_months = 24):\n",
    "    initial_dt = datetime.strptime(initial_year_string, '%Y-%m-%d')\n",
    "    final_dt = datetime.strptime(final_year_string, '%Y-%m-%d')\n",
    "    initial_month_int = initial_dt.year * 12 + initial_dt.month\n",
    "    final_month_int = final_dt.year * 12 + final_dt.month\n",
    "    min_year, min_month = int_to_month(initial_month_int - pre_months)\n",
    "    max_year, max_month = int_to_month(final_month_int + post_months)\n",
    "\n",
    "    string_init = str(int(min_year)) + \"-\" + str(int(min_month))\n",
    "    string_final = str(int(max_year)) + \"-\" + str(int(max_month))\n",
    "    years_range = pd.date_range(string_init, string_final, freq='MS').strftime(\"%Y\").tolist()\n",
    "    months_range = pd.date_range(string_init, string_final, freq='MS').strftime(\"%m\").tolist()\n",
    "\n",
    "    date_range = pd.DataFrame(zip(years_range, months_range))\n",
    "\n",
    "    return date_range\n",
    "\n",
    "def load_store_table(year):\n",
    "    store_path = \"../../../../Data/nielsen_extracts/RMS/\" + year + \"/Annual_Files/stores_\" + year + \".tsv\"\n",
    "    store_table = pd.read_csv(store_path, delimiter = \"\\t\", index_col = \"store_code_uc\")\n",
    "    print(\"Loaded store file of \"+ year)\n",
    "    return store_table\n",
    "\n",
    "def get_product_map(groups):\n",
    "    products_path = \"../../../../Data/nielsen_extracts/RMS/Master_Files/Latest/products.tsv\"\n",
    "    products = pd.read_csv(products_path, delimiter = \"\\t\", encoding = \"cp1252\", header = 0, index_col = [\"upc\",\"upc_ver_uc\"])\n",
    "    int_groups = [int(i) for i in groups]\n",
    "    wanted_products = products[products['product_group_code'].isin(int_groups)]\n",
    "    product_map = wanted_products\n",
    "    return product_map\n",
    "\n",
    "def get_upc_ver_uc_map(year):\n",
    "    upc_ver_path = \"../../../../Data/nielsen_extracts/RMS/\"+str(year)+\"/Annual_Files/rms_versions_\"+str(year)+\".tsv\"\n",
    "    upc_vers = pd.read_csv(upc_ver_path, delimiter = \"\\t\", encoding = \"cp1252\", header = 0, index_col = \"upc\")\n",
    "    upc_vers = upc_vers['upc_ver_uc']\n",
    "    upc_ver_map = upc_vers.to_dict()\n",
    "    return upc_ver_map\n",
    "\n",
    "def get_conversion_map(code, final_unit, method = 'mode'):\n",
    "    # Get in the conversion map -- size1_units, multiplication\n",
    "    master_conversion = pd.read_csv('../../../../All/master/unit_conversion.csv')\n",
    "    assert master_conversion['final_unit'].str.contains(final_unit).any(), \"Cannot find %r as a final_unit\" % final_unit\n",
    "    master_conversion = master_conversion[master_conversion['final_unit'] == final_unit]\n",
    "\n",
    "    these_units = pd.read_csv('../../../../All/m_' + code + '/properties/units_edited.csv')\n",
    "    these_units['conversion'] = 0\n",
    "\n",
    "# Anything that has convert = 1 must be in the master folder\n",
    "    convertible = these_units.loc[these_units.convert == 1].copy()\n",
    "    for this_unit in convertible.units.unique():\n",
    "        assert master_conversion['initial_unit'].str.contains(this_unit).any(), \"Cannot find %r as an initial_unit\" % this_unit\n",
    "        if this_unit in master_conversion.initial_unit.unique():\n",
    "            convert_factor = master_conversion.conversion[master_conversion.initial_unit == this_unit].values\n",
    "            these_units.loc[these_units.units == this_unit, 'conversion'] = convert_factor\n",
    "            convertible.loc[convertible.units == this_unit, 'conversion'] = convert_factor\n",
    "\n",
    "    # Convert the total quantity\n",
    "    convertible['total_quantity'] = convertible['total_quantity'] * convertible['conversion']\n",
    "\n",
    "    # The \"method\" for convert = 0 is mapped to the \"method\" for the convert = 1\n",
    "    # with the largest quantity\n",
    "    where_largest = convertible.total_quantity.idxmax()\n",
    "    if method == 'mode':\n",
    "        base_size = convertible.loc[where_largest]['mode']\n",
    "        other_size = these_units[these_units.convert == 0]['mode']\n",
    "    else:\n",
    "        base_size = convertible.loc[where_largest]['median']\n",
    "        other_size = these_units[these_units.convert == 0]['median']\n",
    "\n",
    "    these_units.conversion[these_units.convert == 0] = convertible.conversion[where_largest] * base_size / other_size\n",
    "    these_units = these_units[['units', 'conversion']]\n",
    "    these_units = these_units.rename(columns = {'units' : 'size1_units'})\n",
    "    these_units = these_units.set_index('size1_units')\n",
    "\n",
    "    conversion_map = these_units.to_dict()\n",
    "    return conversion_map\n",
    "def load_chunked_year_module_movement_table(year, group, module, path = ''):\n",
    "    if path == '':\n",
    "        path = \"../../../../Data/nielsen_extracts/RMS/\" + year + \"/Movement_Files/\" + group + \"_\" + year + \"/\" + module + \"_\" + year + \".tsv\"\n",
    "    assert os.path.exists(path), \"File does not exist: %r\" % path\n",
    "    table = pd.read_csv(path, delimiter = \"\\t\", chunksize = 10000000)\n",
    "    return table\n",
    "\n",
    "def aggregate_movement(code, years, groups, modules, month_or_quarter, conversion_map, merger_start_date, merger_stop_date, market_size_scale = 1.5, pre_months = 24, post_months = 24):\n",
    "\n",
    "# Get the relevant range\n",
    "    stop_dt = datetime.strptime(merger_stop_date, '%Y-%m-%d')\n",
    "    start_dt = datetime.strptime(merger_start_date, '%Y-%m-%d')\n",
    "    stop_month_int = stop_dt.year * 12 + stop_dt.month\n",
    "    start_month_int = start_dt.year * 12 + start_dt.month\n",
    "\n",
    "    min_year, min_month = aux.int_to_month(start_month_int - pre_months)\n",
    "    max_year, max_month = aux.int_to_month(stop_month_int + post_months)\n",
    "    min_quarter = np.ceil(min_month/3)\n",
    "    max_quarter = np.ceil(max_month/3)\n",
    "\n",
    "    #manual fix for baby strained food\n",
    "    if ((code=='1817013020_3') & (max_year > 2008)):\n",
    "        max_year = 2008\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2008, years))\n",
    "\n",
    "    #manual fix for bread\n",
    "    if ((code=='2203820020_1') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for buns\n",
    "    if ((code=='2203820020_2') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for rolls\n",
    "    if ((code=='2203820020_3') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for pies\n",
    "    if ((code=='2203820020_8') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for bakery remaining\n",
    "    if ((code=='2203820020_10') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for cheesecake\n",
    "    if ((code=='2203820020_11') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "    #manual fix for biscuits\n",
    "    if ((code=='2203820020_12') & (max_year > 2012)):\n",
    "        max_year = 2012\n",
    "        max_month = 12\n",
    "        max_quarter = 4\n",
    "        years = list(filter(lambda x: int(x) <= 2012, years))\n",
    "\n",
    "        #manual fix for RBC_Bread\n",
    "    if ((code=='2033113020_2') & (min_year < 2007)):\n",
    "        min_year = 2007\n",
    "        min_month = 1\n",
    "        min_quarter = 1\n",
    "        years = list(filter(lambda x: int(x) >= 2007, years))\n",
    "\n",
    "        #manual fix for RBC_Cake\n",
    "    if ((code=='2033113020_3') & (min_year < 2007)):\n",
    "        min_year = 2007\n",
    "        min_month = 1\n",
    "        min_quarter = 1\n",
    "        years = list(filter(lambda x: int(x) >= 2007, years))\n",
    "\n",
    "        #manual fix for Headache pills\n",
    "    if ((code=='2373087020_1') & (min_year < 2010)):\n",
    "        min_year = 2010\n",
    "        min_month = 1\n",
    "        min_quarter = 1\n",
    "        years = list(filter(lambda x: int(x) >= 2010, years))\n",
    "\n",
    "        #manual fix for School and Office Supplies\n",
    "    if ((code=='2363232020_4') & (min_year < 2010)):\n",
    "        min_year = 2010\n",
    "        min_month = 1\n",
    "        min_quarter = 1\n",
    "        years = list(filter(lambda x: int(x) >= 2010, years))\n",
    "\n",
    "    area_time_upc_list = []\n",
    "    product_map = get_product_map(list(set(groups)))\n",
    "    add_from_map = ['brand_code_uc', 'brand_descr', 'multi', 'size1_units', 'size1_amount']\n",
    "    aggregation_function = {'week_end' : 'first', 'units' : 'sum', 'prmult' : 'mean', 'price' : 'mean', 'feature' : 'first', 'display' : 'first', 'store_code_uc' : 'first', 'sales' : 'sum', 'module' : 'first'}\n",
    "\n",
    "    for year in years:\n",
    "        store_table = load_store_table(year)\n",
    "        store_map = store_table.to_dict()\n",
    "        dma_map = store_map['dma_code']\n",
    "        upc_ver_map = get_upc_ver_uc_map(year)\n",
    "\n",
    "        for group, module in zip(groups, modules):\n",
    "            movement_table = load_chunked_year_module_movement_table(year, group, module)\n",
    "\n",
    "            for data_chunk in tqdm(movement_table):\n",
    "                data_chunk['year'] = np.floor(data_chunk['week_end']/10000)\n",
    "                data_chunk['year'] = data_chunk['year'].astype(int)\n",
    "                if month_or_quarter == \"month\":\n",
    "                    data_chunk[month_or_quarter] = np.floor((data_chunk['week_end'] % 10000)/100)\n",
    "                    data_chunk[month_or_quarter] = data_chunk[month_or_quarter].astype(int)\n",
    "\n",
    "                    if int(year) == min_year:\n",
    "                        data_chunk = data_chunk[data_chunk.month >= min_month]\n",
    "                    elif int(year) == max_year:\n",
    "                        data_chunk = data_chunk[data_chunk.month <= max_month]\n",
    "                elif month_or_quarter == \"quarter\":\n",
    "                    data_chunk[month_or_quarter] = np.ceil(np.floor((data_chunk['week_end'] % 10000)/100)/3)\n",
    "                    data_chunk[month_or_quarter] = data_chunk[month_or_quarter].astype(int)\n",
    "                    if int(year) == min_year:\n",
    "                        data_chunk = data_chunk[data_chunk.quarter >= min_quarter]\n",
    "                    elif int(year) == max_year:\n",
    "                        data_chunk = data_chunk[data_chunk.quarter <= max_quarter]\n",
    "\n",
    "                data_chunk['dma_code'] = data_chunk['store_code_uc'].map(dma_map)\n",
    "                data_chunk['sales'] = data_chunk['price'] * data_chunk['units'] / data_chunk['prmult']\n",
    "                data_chunk['module'] = int(module)\n",
    "                data_chunk['upc_ver_uc'] = data_chunk['upc'].map(upc_ver_map)\n",
    "                area_time_upc = data_chunk.groupby(['year', month_or_quarter, 'upc', 'upc_ver_uc', 'dma_code'], as_index = False).aggregate(aggregation_function).reindex(columns = data_chunk.columns)\n",
    "                area_time_upc_list.append(area_time_upc)\n",
    "\n",
    "    area_time_upc = pd.concat(area_time_upc_list)\n",
    "    area_time_upc = area_time_upc.groupby(['year', month_or_quarter, 'upc', 'upc_ver_uc', 'dma_code'], as_index = False).aggregate(aggregation_function).reindex(columns = area_time_upc.columns)\n",
    "    area_time_upc = area_time_upc.join(product_map[add_from_map], on=['upc','upc_ver_uc'], how='left')\n",
    "    area_time_upc = clean_data(code, area_time_upc)\n",
    "    area_time_upc['conversion'] = area_time_upc['size1_units'].map(conversion_map['conversion'])\n",
    "    area_time_upc['volume'] = area_time_upc['units'] * area_time_upc['size1_amount'] * area_time_upc['multi'] * area_time_upc['conversion']\n",
    "    area_time_upc['prices'] = area_time_upc['sales'] / area_time_upc['volume']\n",
    "\n",
    "    #area_time_upc.drop(['week_end','store_code_uc'], axis=1, inplace=True)\n",
    "\n",
    "    # Normalize the prices by the CPI.  Let January 2010 = 1.\n",
    "    area_time_upc = aux.adjust_inflation(area_time_upc, ['prices', 'sales'], month_or_quarter)\n",
    "\n",
    "    # Get the market sizes here, by summing volume within dma-time and then taking 1.5 times max within-dma\n",
    "    short_area_time_upc = area_time_upc[['dma_code', 'year', month_or_quarter, 'volume', 'sales']]\n",
    "    market_sizes = short_area_time_upc.groupby(['dma_code', 'year', month_or_quarter]).sum()\n",
    "    market_sizes['market_size'] = market_size_scale * market_sizes['volume'].groupby('dma_code').transform('max')\n",
    "    market_sizes = market_sizes.rename(columns = {'sales': 'total_sales', 'volume' : 'total_volume'})\n",
    "\n",
    "    # Save the output if this is month\n",
    "    if month_or_quarter == 'month':\n",
    "        market_sizes.to_csv('../../../../All/m_' + code + '/intermediate/market_sizes.csv', sep = ',', encoding = 'utf-8')\n",
    "\n",
    "    # Shares = volume / market size.  Map market sizes back and get shares.\n",
    "    area_time_upc = area_time_upc.join(market_sizes.drop('total_volume', axis=1), on = ['dma_code', 'year', month_or_quarter])\n",
    "    area_time_upc['shares'] = area_time_upc['volume'] / area_time_upc['market_size']\n",
    "\n",
    "    return area_time_upc, store_map\n",
    "\n",
    "def store_aggregation(code):\n",
    "    info_dict = parse_info(code)\n",
    "    info_dict.keys()\n",
    "    final_unit = info_dict['FinalUnits']\n",
    "\n",
    "    groups, modules = aux.get_groups_and_modules(info_dict[\"MarketDefinition\"])\n",
    "    years = aux.get_years(info_dict[\"DateAnnounced\"], info_dict[\"DateCompleted\"])\n",
    "\n",
    "    # make conversion map\n",
    "    conversion_map = get_conversion_map(code, info_dict[\"FinalUnits\"])\n",
    "        \n",
    "    area_month_upc, store_map = aggregate_movement(code, years, groups, modules, \"month\", conversion_map, info_dict[\"DateAnnounced\"], info_dict[\"DateCompleted\"])\n",
    "\n",
    "    #area_month_upc = pd.DataFrame.from_records(area_month_upc, columns = ['store_code_uc', 'upc', 'units', 'prmult', 'price', 'feature','display', \n",
    "     #   'year', 'month', 'dma_code', 'sales', 'module', 'upc_ver_uc','brand_code_uc', 'brand_descr', 'multi', 'size1_units', 'size1_amount', \n",
    "      #  'conversion', 'volume', 'prices', 'total_sales', 'market_size','shares'])\n",
    "\n",
    "    print(type(area_month_upc))\n",
    "\n",
    "    #area_month_upc = pd.DataFrame.from_records(area_month_upc)\n",
    "\n",
    "    # creating area_month_upc file\n",
    "    area_month_upc = area_month_upc[['store_code_uc', 'upc', 'year', 'month', 'sales', 'dma_code', 'volume']]\n",
    "\n",
    "    # loading stores\n",
    "    #store_map.to_csv(\"store_map.csv\")\n",
    "\n",
    "    # inserting store type\n",
    "    area_month_upc.insert(1, \"channel_code\", area_month_upc[\"store_code_uc\"].map(store_map[\"channel_code\"]))\n",
    "    area_month_upc.insert(1, \"parent_code\", area_month_upc[\"store_code_uc\"].map(store_map[\"parent_code\"]))\n",
    "\n",
    "    area_month_upc.to_csv('m_' + code + '/area_month.csv')\n",
    "\n",
    "    area_month_upc = area_month_upc.groupby(['channel_code','upc','year','month']).agg({'sales': 'sum', 'volume': 'sum'})\n",
    "    area_month_upc = area_month_upc.pivot_table(index = ['upc','year','month'], columns = 'channel_code', values = ['sales','volume'], fill_value = 0).reset_index()\n",
    "\n",
    "    area_month_upc.to_csv('m_' + code + '/area_month_with_channelcodes.csv')\n",
    "    \n",
    "    return area_month_upc\n",
    "\n",
    "def append_owners(code, df, month_or_quarter,add_dhhi = False):\n",
    "    # Load list of UPCs and brands\n",
    "    upcs = pd.read_csv('../../../../All/m_' + code + '/intermediate/upcs.csv', delimiter = ',', index_col = 'upc')\n",
    "    upcs = upcs['brand_code_uc']\n",
    "    upc_map = upcs.to_dict()\n",
    "\n",
    "# # Map brands to dataframe (by UPC)\n",
    "    df['brand_code_uc'] = df['upc'].map(upc_map)\n",
    "\n",
    "# Load ownership assignments\n",
    "    brand_to_owner = pd.read_csv('../../../../All/m_' + code + '/properties/ownership.csv', delimiter = ',', index_col = 'brand_code_uc')\n",
    "\n",
    "# Assign min/max year and month when listed as zero in ownership mapping\n",
    "    min_year = df['year'].min()\n",
    "    max_year = df['year'].max()\n",
    "\n",
    "    if month_or_quarter == 'month':\n",
    "        min_month = df.loc[df['year']==min_year,'month'].min()\n",
    "        max_month = df.loc[df['year']==max_year,'month'].max()\n",
    "    elif month_or_quarter == 'quarter':\n",
    "        min_month = (3*(df.loc[df['year']==min_year,'quarter']-1)+1).min()\n",
    "        max_month = (3*df.loc[df['year']==max_year,'quarter']).max()\n",
    "\n",
    "    # Remove Onwership that starts later than the latest time in the dataframe\n",
    "    brand_to_owner = brand_to_owner[(brand_to_owner['start_year'] < max_year) | ((brand_to_owner['start_year'] == max_year)&(brand_to_owner['start_month'] <= max_month))]\n",
    "    # Remove Onwership that ends earlier than the earliest time in the dataframe\n",
    "    brand_to_owner = brand_to_owner[(brand_to_owner['end_year'] > min_year) | ((brand_to_owner['end_year'] == min_year)&(brand_to_owner['end_month'] >= min_month)) | (brand_to_owner['end_year'] == 0)]\n",
    "\n",
    "    brand_to_owner.loc[(brand_to_owner['start_month']==0) | (brand_to_owner['start_year']<min_year) | ((brand_to_owner['start_year']==min_year)&(brand_to_owner['start_month']<min_month)),'start_month'] = min_month\n",
    "    brand_to_owner.loc[(brand_to_owner['start_year']==0) | (brand_to_owner['start_year']<min_year),'start_year'] = min_year\n",
    "    brand_to_owner.loc[(brand_to_owner['end_month']==0) | (brand_to_owner['end_year']>max_year) | ((brand_to_owner['end_year']==max_year)&(brand_to_owner['end_month']>max_month)),'end_month'] = max_month\n",
    "    brand_to_owner.loc[(brand_to_owner['end_year']==0) | (brand_to_owner['end_year']>max_year),'end_year'] = max_year\n",
    "\n",
    "    # Throw error if (1) dates don't span the entirety of the sample period or\n",
    "    # (2) ownership dates overlap\n",
    "    brand_to_owner_test = brand_to_owner.copy()\n",
    "    brand_to_owner_test = brand_to_owner_test.sort_values(by=['brand_code_uc', 'start_year', 'start_month'])\n",
    "    \n",
    "    if month_or_quarter == 'month':\n",
    "        min_date = pd.to_datetime(dict(year=df.year, month=df.month, day=1)).min()\n",
    "        max_date = pd.to_datetime(dict(year=df.year, month=df.month, day=1)).max()\n",
    "        brand_to_owner_test['start_date_test'] = pd.to_datetime(dict(year=brand_to_owner_test.start_year, month=brand_to_owner_test.start_month, day=1))\n",
    "        brand_to_owner_test['end_date_test'] = pd.to_datetime(dict(year=brand_to_owner_test.end_year, month=brand_to_owner_test.end_month, day=1))\n",
    "    elif month_or_quarter == 'quarter':\n",
    "        min_date = pd.to_datetime(dict(year=df.year, month=3*(df.quarter-1)+1, day=1)).min()\n",
    "        max_date = pd.to_datetime(dict(year=df.year, month=3*df.quarter, day=1)).max()\n",
    "        brand_to_owner_test.loc[:,'start_month'] = 3*(np.ceil(brand_to_owner_test['start_month']/3)-1)+1\n",
    "        brand_to_owner_test.loc[:,'end_year'] = np.where(3*(np.floor(brand_to_owner_test.end_month/3)) > 0, brand_to_owner_test.end_year, brand_to_owner_test.end_year - 1)\n",
    "        brand_to_owner_test.loc[:,'end_month'] = np.where(3*(np.floor(brand_to_owner_test.end_month/3)) > 0, 3*(np.floor(brand_to_owner_test.end_month/3)), 12)\n",
    "        brand_to_owner_test['start_date_test'] = pd.to_datetime(dict(year=brand_to_owner_test.start_year, month=brand_to_owner_test.start_month, day=1))\n",
    "        brand_to_owner_test['end_date_test'] = pd.to_datetime(dict(year=brand_to_owner_test.end_year, month=brand_to_owner_test.end_month, day=1))\n",
    "\n",
    "    brand_dates = brand_to_owner_test.groupby('brand_code_uc')[['start_date_test', 'end_date_test']].agg(['min', 'max'])\n",
    "    if ((brand_dates.start_date_test['min']!=min_date).sum() + (brand_dates.end_date_test['max']!=max_date).sum() > 0):\n",
    "        print('Ownership definitions does not span the entire sample period:')\n",
    "        for index, row in brand_dates.iterrows():\n",
    "            if row.start_date_test['min'] != min_date or row.end_date_test['max'] != max_date:\n",
    "                print(index)\n",
    "                print('start_date: ', row.start_date_test['min'])\n",
    "                print('end_date: ', row.end_date_test['max'])\n",
    "\n",
    "    brand_to_owner_test['owner_num'] = brand_to_owner_test.groupby('brand_code_uc').cumcount()+1\n",
    "    max_num_owner = brand_to_owner_test['owner_num'].max()\n",
    "    brand_to_owner_test = brand_to_owner_test.set_index('owner_num',append=True)\n",
    "    brand_to_owner_test = brand_to_owner_test.unstack('owner_num')\n",
    "    brand_to_owner_test.columns = ['{}_{}'.format(var, num) for var, num in brand_to_owner_test.columns]\n",
    "\n",
    "    for ii in range(2,max_num_owner+1):\n",
    "        overlap_or_gap = (brand_to_owner_test['start_year_' + str(ii)] < brand_to_owner_test['end_year_' + str(ii-1)]) | \\\n",
    "            ((brand_to_owner_test['start_year_' + str(ii)] == brand_to_owner_test['end_year_' + str(ii-1)]) & \\\n",
    "            (brand_to_owner_test['start_month_' + str(ii)] != (brand_to_owner_test['end_month_' + str(ii-1)] + 1))) | \\\n",
    "            ((brand_to_owner_test['start_year_' + str(ii)] > brand_to_owner_test['end_year_' + str(ii-1)]) & \\\n",
    "            ((brand_to_owner_test['start_month_' + str(ii)] != 1) | (brand_to_owner_test['end_month_' + str(ii-1)] != 12)))\n",
    "        if overlap_or_gap.sum() > 0:\n",
    "            brand_to_owner_test['overlap'] = overlap_or_gap\n",
    "            indices = brand_to_owner_test[brand_to_owner_test['overlap'] != 0].index.tolist()\n",
    "            for index in indices:\n",
    "                print(brand_to_owner_test.loc[index])\n",
    "            raise Exception('There are gaps or overlap in the ownership mapping.')\n",
    "\n",
    "    # Merge on brand and date intervals\n",
    "    if month_or_quarter == 'month':\n",
    "        brand_to_owner['start_date'] = pd.to_datetime(dict(year=brand_to_owner.start_year, month=brand_to_owner.start_month, day=1))\n",
    "        brand_to_owner['end_date'] = pd.to_datetime(dict(year=brand_to_owner.end_year, month=brand_to_owner.end_month, day=1))\n",
    "        df['date'] = pd.to_datetime(dict(year=df.year, month=df.month, day=1))\n",
    "        if add_dhhi:\n",
    "            sqlcode = '''\n",
    "            select df.upc, df.year, df.month, df.shares, df.dma_code, df.brand_code_uc, brand_to_owner.owner\n",
    "            from df\n",
    "            inner join brand_to_owner on df.brand_code_uc=brand_to_owner.brand_code_uc AND df.date >= brand_to_owner.start_date AND df.date <= brand_to_owner.end_date\n",
    "            '''\n",
    "        else:\n",
    "            sqlcode = '''\n",
    "            select df.upc, df.year, df.month, df.prices, df.shares, df.volume, df.dma_code, df.brand_code_uc, df.sales, brand_to_owner.owner\n",
    "            from df\n",
    "            inner join brand_to_owner on df.brand_code_uc=brand_to_owner.brand_code_uc AND df.date >= brand_to_owner.start_date AND df.date <= brand_to_owner.end_date\n",
    "            '''\n",
    "    elif month_or_quarter == 'quarter':\n",
    "        brand_to_owner.loc[:,'start_month'] = 3*(np.ceil(brand_to_owner['start_month']/3)-1)+1\n",
    "        brand_to_owner.loc[:,'end_year'] = np.where(3*(np.floor(brand_to_owner.end_month/3)) > 0, brand_to_owner.end_year, brand_to_owner.end_year - 1)\n",
    "        brand_to_owner.loc[:,'end_month'] = np.where(3*(np.floor(brand_to_owner.end_month/3)) > 0, 3*(np.floor(brand_to_owner.end_month/3)), 12)\n",
    "        brand_to_owner['start_date'] = pd.to_datetime(dict(year=brand_to_owner.start_year, month=brand_to_owner.start_month, day=1))\n",
    "        brand_to_owner['end_date'] = pd.to_datetime(dict(year=brand_to_owner.end_year, month=brand_to_owner.end_month, day=1))\n",
    "        df['date'] = pd.to_datetime(dict(year=df.year, month=3*(df.quarter-1)+1, day=1))\n",
    "        if add_dhhi:\n",
    "            sqlcode = \"\"\"\n",
    "            select \n",
    "                df.upc, df.year, df.quarter, df.shares, df.dma_code, df.brand_code_uc, brand_to_owner.owner\n",
    "            from \n",
    "                df\n",
    "            inner join \n",
    "                brand_to_owner \n",
    "                    on df.brand_code_uc=brand_to_owner.brand_code_uc AND df.date >= brand_to_owner.start_date AND df.date <= brand_to_owner.end_date\n",
    "            \"\"\"\n",
    "        else:\n",
    "            sqlcode = \"\"\"\n",
    "            select \n",
    "                df.upc, df.year, df.quarter, df.prices, df.shares, df.volume, df.dma_code, df.brand_code_uc, df.sales, brand_to_owner.owner\n",
    "            from \n",
    "                df\n",
    "            inner join \n",
    "                brand_to_owner \n",
    "                    on df.brand_code_uc=brand_to_owner.brand_code_uc AND df.date >= brand_to_owner.start_date AND df.date <= brand_to_owner.end_date\n",
    "            \"\"\"\n",
    "    df_own = sqldf(sqlcode,locals())\n",
    "\n",
    "    return df_own\n",
    "\n",
    "def get_parties(info_str):\n",
    "    all_parties = re.finditer('{(.*?)}', info_str, re.DOTALL)\n",
    "    merging_parties = []\n",
    "    for i in all_parties:\n",
    "        merging_parties.append(i.group(1).strip())\n",
    "    return merging_parties\n",
    "\n",
    "\n",
    "def table_1(code):\n",
    "    \n",
    "    # must have 4 ../../../.. because i'm inside a folder inside Main\n",
    "    # opening data_month file\n",
    "    df = (pd.read_csv('../../../../All/m_' + code + '/intermediate/data_month.csv'))\n",
    "    \n",
    "    \n",
    "    ### Part 1: making df complete with year, months and all dma_codes exhaustive\n",
    "    ### note - df_own does NOT have all dates, only dates which the upc-year-month sales > 0!!\n",
    "    \n",
    "    # create list of all year month combinations\n",
    "    info_dict = parse_info(code)\n",
    "    date_range = get_date_range(info_dict['DateAnnounced'], info_dict['DateCompleted'], pre_months = 24, post_months = 24)\n",
    "    \n",
    "    # calculate number of rows there should be\n",
    "    upcs = (pd.read_csv('../../../../All/m_' + code + '/intermediate/upcs.csv', delimiter = ','))['upc']\n",
    "    #total_rows = len(upcs)*len(date_range)\n",
    "    #total_columns = len(pivoted.columns)\n",
    "    \n",
    "    # take upc-year-month-dma combinations now\n",
    "    repeated_upcs = (pd.concat([upcs]*len(date_range))).sort_values() #ignore_index = True\n",
    "    repeated_upcs.columns = ['upc']\n",
    "    repeated_upcs = repeated_upcs.reset_index(drop=True)\n",
    "\n",
    "    repeated_dates = pd.concat([date_range]*len(upcs))\n",
    "    repeated_dates.columns = ['year', 'month']\n",
    "    repeated_dates = repeated_dates.reset_index(drop=True)\n",
    "\n",
    "    unique_dma_codes = pd.DataFrame(df['dma_code'].unique())\n",
    "    repeated_dmas = pd.concat([unique_dma_codes]*len(upcs)*len(date_range))\n",
    "    repeated_dmas.columns = ['dma_code']\n",
    "    repeated_dmas = repeated_dmas.reset_index(drop=True)\n",
    "\n",
    "    # creating the empty dataframe with all upc-year-month-dma combinations\n",
    "    empty_to_merge = pd.concat([pd.concat([repeated_upcs, repeated_dates], axis= 1)]*len(unique_dma_codes))\n",
    "    empty_to_merge = empty_to_merge.reset_index(drop=True)\n",
    "    empty_to_merge = empty_to_merge.sort_values(by = ['upc', 'year', 'month'])\n",
    "    empty_to_merge.insert(1, 'dma_code', repeated_dmas)\n",
    "    empty_to_merge = empty_to_merge.apply(pd.to_numeric)\n",
    "\n",
    "    # actual merging with incomplete df, filling in 0's for all spots where no sales\n",
    "    empty_to_merge_dma_full = pd.merge(df, empty_to_merge, how = \"right\", on = ['upc', 'dma_code','year', 'month'])\n",
    "    empty_to_merge_dma_full.fillna(0, inplace=True)\n",
    "\n",
    "    empty_to_merge_dma_full = empty_to_merge_dma_full.apply(pd.to_numeric)\n",
    "\n",
    "    \n",
    "    ### Part 2: extracting ownership using full df with all upc-year-month-dma combinations\n",
    "    df_own = append_owners(code, empty_to_merge_dma_full, 'month')\n",
    "\n",
    "    # pivoting the dmas and having sales and volume for each upc year month\n",
    "    pivoted = df_own.pivot_table(index = ['upc','year','month','owner'], columns = 'dma_code', values = ['volume','sales']).reset_index()\n",
    "\n",
    "    # filling in for 0\n",
    "    pivoted.fillna(0, inplace=True)\n",
    "    \n",
    "    ### Part 3: Additional info\n",
    "    \n",
    "    #Total sales of the product in that quarter across the entire US.  Do volume and dollar sales.  For UPCs that aren't sold, it will be 0.\n",
    "    pivoted.loc[:,'total_sales'] = pivoted[['sales']].sum(axis=1)\n",
    "    pivoted.loc[:,'total_volume'] = pivoted[['volume']].sum(axis=1)\n",
    "    \n",
    "    # dummy for whether product is sold in that month or not in the US\n",
    "    pivoted['sold_in_usa'] = 0\n",
    "    pivoted.loc[pivoted.total_sales != 0, 'sold_in_usa'] = 1\n",
    "    \n",
    "    # dummies for whether the product is involved in a merger and whether it's post-merger\n",
    "    # create column of zeroes\n",
    "    pivoted['merging_party'] = 0\n",
    "    pivoted['post_merger'] = 0\n",
    "\n",
    "    # extract merging parties\n",
    "    merging_parties = get_parties(info_dict[\"MergingParties\"])\n",
    "    \n",
    "    #assign 1's if owner = merging parties\n",
    "    pivoted.loc[pivoted['owner'].isin(merging_parties), 'merging_party'] = 1\n",
    "    \n",
    "    # extracting year and month of date completed\n",
    "    year = int((info_dict['DateCompleted'])[:4])\n",
    "    month = int((info_dict['DateCompleted'])[5:7])\n",
    "    \n",
    "    # setting = 1 if month and year are greater than date completed for the merging parties\n",
    "    pivoted.loc[(pivoted['merging_party'] == 1) & (pivoted['year'] >= year) & (pivoted['month'] >= month), 'post_merger'] = 1\n",
    "    \n",
    "    pivoted.loc[(pivoted['merging_party'] == 1) & (pivoted['year'] > year), 'post_merger'] = 1\n",
    "    pivoted.loc[(pivoted['merging_party'] == 1) & (pivoted['year'] == year) & (pivoted['month'] >= month), 'post_merger'] = 1\n",
    "    \n",
    "    # export to csv\n",
    "    pivoted.to_csv('m_' + code +'/pivoted_data.csv', sep = ',', encoding = 'utf-8')\n",
    "    \n",
    "    log_out = open('try_1.log', 'w')\n",
    "    log_err = open('try_1.err', 'w')\n",
    "    sys.stdout = log_out\n",
    "    sys.stderr = log_err\n",
    "    \n",
    "    return pivoted\n",
    "\n",
    "codes = ['1924129020_1', '2641303020_8', '2823116020_9']\n",
    "\n",
    "for code in codes:\n",
    "    os.mkdir('m_' + code)\n",
    "    pivoted = table_1(code)\n",
    "    area_month_upc = store_aggregation(code)\n",
    "    final_table = pd.merge(pivoted, area_month_upc, how = \"right\", on = ['upc', 'year', 'month']).fillna(0)\n",
    "    final_table.to_csv('m_' + code + '/final_table.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
